{
  "accelerator_frequency": "1980000 MHz",
  "accelerator_host_interconnect": "N/A",
  "accelerator_interconnect": "N/A",
  "accelerator_interconnect_topology": "",
  "accelerator_memory_capacity": "79.20880126953125 GB",
  "accelerator_memory_configuration": "N/A",
  "accelerator_model_name": "NVIDIA H100 80GB HBM3",
  "accelerator_on-chip_memories": "",
  "accelerators_per_node": 5,
  "cooling": "air",
  "division": "open",
  "framework": "TensorRT",
  "host_memory_capacity": "773G",
  "host_memory_configuration": "undefined",
  "host_network_card_count": "1",
  "host_networking": "Gig Ethernet",
  "host_networking_topology": "N/A",
  "host_processor_caches": "L1d cache: 6 MiB, L1i cache: 6 MiB, L2 cache: 192 MiB, L3 cache: 2.3 GiB",
  "host_processor_core_count": "96",
  "host_processor_frequency": "3715.4290",
  "host_processor_interconnect": "",
  "host_processor_model_name": "AMD EPYC 9684X 96-Core Processor",
  "host_processors_per_node": "2",
  "host_storage_capacity": "9.0T",
  "host_storage_type": "SSD",
  "hw_notes": "",
  "number_of_nodes": "1",
  "operating_system": "Ubuntu 20.04 (linux-5.14.0-427.33.1.el9_4.x86_64-glibc2.31)",
  "other_software_stack": "Python: 3.8.10, LLVM-10.0.0 , CUDA 12.2",
  "status": "available",
  "submitter": "peking1",
  "sw_notes": "Automated by MLCommons CM v3.4.1. ",
  "system_name": "mlperf_inference_lry_40",
  "system_type": "datacenter",
  "system_type_detail": "edge server"
}